{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TB detection using Tensor Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version 1.3.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from models.lotenet import loTeNet\n",
    "from torchvision import transforms, datasets\n",
    "import pdb\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from models.Densenet import *\n",
    "from utils.tools import *\n",
    "import argparse\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print('Using PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\nimport numpy as np\\nimport shutil\\nimport random\\n\\n# # Creating Train / Val / Test folders (One time use)\\nroot_dir = '/home/mashjunior/loTeNet_pytorch/TBCXRDatabase/TB_Chest_Radiography_Database/'\\nclasses_dir = ['Normal', 'Tuberculosis']\\n\\nval_ratio = 0.15\\ntest_ratio = 0.05\\n\\nfor cls in classes_dir:\\n    os.makedirs(root_dir +'/train' + cls)\\n    os.makedirs(root_dir +'/val' + cls)\\n    os.makedirs(root_dir +'/test' + cls)\\n\\n\\n    # Creating partitions of the data after shuffeling\\n    src = root_dir + cls # Folder to copy images from\\n\\n    allFileNames = os.listdir(src)\\n    np.random.shuffle(allFileNames)\\n    train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\\n                                                              [int(len(allFileNames)* (1 - val_ratio + test_ratio)), \\n                                                               int(len(allFileNames)* (1 - test_ratio))])\\n\\n\\n    train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\\n    val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]\\n    test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\\n\\n    print('Total images: ', len(allFileNames))\\n    print('Training: ', len(train_FileNames))\\n    print('Validation: ', len(val_FileNames))\\n    print('Testing: ', len(test_FileNames))\\n\\n    # Copy-pasting images\\n    for name in train_FileNames:\\n        shutil.copy(name, root_dir +'/train' + cls)\\n\\n    for name in val_FileNames:\\n        shutil.copy(name, root_dir +'/val' + cls)\\n\\n    for name in test_FileNames:\\n        shutil.copy(name, root_dir +'/test' + cls)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# # Creating Train / Val / Test folders (One time use)\n",
    "root_dir = '/home/mashjunior/loTeNet_pytorch/TBCXRDatabase/TB_Chest_Radiography_Database/'\n",
    "classes_dir = ['Normal', 'Tuberculosis']\n",
    "\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.05\n",
    "\n",
    "for cls in classes_dir:\n",
    "    os.makedirs(root_dir +'/train' + cls)\n",
    "    os.makedirs(root_dir +'/val' + cls)\n",
    "    os.makedirs(root_dir +'/test' + cls)\n",
    "\n",
    "\n",
    "    # Creating partitions of the data after shuffeling\n",
    "    src = root_dir + cls # Folder to copy images from\n",
    "\n",
    "    allFileNames = os.listdir(src)\n",
    "    np.random.shuffle(allFileNames)\n",
    "    train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                                              [int(len(allFileNames)* (1 - val_ratio + test_ratio)), \n",
    "                                                               int(len(allFileNames)* (1 - test_ratio))])\n",
    "\n",
    "\n",
    "    train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
    "    val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]\n",
    "    test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\n",
    "\n",
    "    print('Total images: ', len(allFileNames))\n",
    "    print('Training: ', len(train_FileNames))\n",
    "    print('Validation: ', len(val_FileNames))\n",
    "    print('Testing: ', len(test_FileNames))\n",
    "\n",
    "    # Copy-pasting images\n",
    "    for name in train_FileNames:\n",
    "        shutil.copy(name, root_dir +'/train' + cls)\n",
    "\n",
    "    for name in val_FileNames:\n",
    "        shutil.copy(name, root_dir +'/val' + cls)\n",
    "\n",
    "    for name in test_FileNames:\n",
    "        shutil.copy(name, root_dir +'/test' + cls)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['normal', 'tuberculosis']\n",
    "root_dir = '/home/mashjunior/loTeNet_pytorch/TBCXRDatabase/TB_Chest_Radiography_Database/loTenet_data'\n",
    "source_dirs = ['normal' ,'tuberculosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXRayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dirs,transform):\n",
    "        def get_images(class_name):\n",
    "            images = [x for x in os.listdir(image_dirs[class_name]) if x.lower().endswith('png')]\n",
    "            print(f'Found {len(images)}{class_name}')\n",
    "            return images\n",
    "        self.images={}\n",
    "        self.class_names=['normal','tuberculosis']\n",
    "        for c in self.class_names:\n",
    "            self.images[c]=get_images(c)\n",
    "        self.image_dirs=image_dirs\n",
    "        self.transform=transform\n",
    "    def __len__(self):\n",
    "        return sum([len(self.images[c]) for c in self.class_names])\n",
    "    def __getitem__(self, index):\n",
    "        class_name=random.choice(self.class_names)\n",
    "        index=index%len(self.images[class_name])\n",
    "        image_name=self.images[class_name][index]\n",
    "        image_path =os.path.join(self.image_dirs[class_name], image_name)\n",
    "        image=Image.open(image_path).convert('L')\n",
    "        return self.transform(image), self.class_names.index(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globally load device identifier\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "     ### Evaluation funcntion for validation/testing\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vl_acc = 0.\n",
    "        vl_loss = 0.\n",
    "        labelsNp = np.zeros(1)\n",
    "        predsNp = np.zeros(1)\n",
    "        model.eval()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(loader):\n",
    "\n",
    "            inputs = inputs.to(device).type(dtype=torch.float)\n",
    "            labels = labels.to(device).type(dtype=torch.float)\n",
    "            labelsNp = np.concatenate((labelsNp, labels.cpu().numpy()))\n",
    "\n",
    "            # Inference\n",
    "            scores = torch.sigmoid(model(inputs)).type(dtype=torch.float)\n",
    "\n",
    "            preds = scores\n",
    "            loss = loss_fun(scores, labels)\n",
    "            predsNp = np.concatenate((predsNp, preds.cpu().numpy()))\n",
    "            vl_loss += loss.item()\n",
    "\n",
    "        # Compute AUC over the full (valid/test) set\n",
    "        vl_acc = computeAuc(labelsNp[1:],predsNp[1:])\n",
    "        vl_loss = vl_loss/len(loader)\n",
    "\n",
    "    return vl_acc, vl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellaneous initialization\n",
    "torch.manual_seed(1)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--dense_net'], dest='dense_net', nargs=0, const=True, default=False, type=None, choices=None, help='Using Dense Net model', metavar=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument('--num_epochs', type=int, default=10, help='Number of training epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n",
    "parser.add_argument('--lr', type=float, default=5e-4, help='Learning rate')\n",
    "parser.add_argument('--l2', type=float, default=0, help='L2 regularisation')\n",
    "parser.add_argument('--aug', action='store_true', default=False, help='Use data augmentation')\n",
    "parser.add_argument('--data_path', type=str, default=root_dir,help='Path to data.')\n",
    "parser.add_argument('--bond_dim', type=int, default=5, help='MPS Bond dimension')\n",
    "parser.add_argument('--nChannel', type=int, default=1, help='Number of input channels')\n",
    "parser.add_argument('--dense_net', action='store_true', default=False, help='Using Dense Net model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = args.batch_size\n",
    "\n",
    "# LoTeNet parameters\n",
    "adaptive_mode = False \n",
    "periodic_bc   = False\n",
    "\n",
    "kernel = 2 # Stride along spatial dimensions\n",
    "output_dim = 1 # output dimension\n",
    " \n",
    "feature_dim = 2\n",
    "\n",
    "#logFile = time.strftime(\"%Y%m%d_%H_%M\")+'.txt'\n",
    "#makeLogFile(logFile)\n",
    "\n",
    "normTensor = 0.5*torch.ones(args.nChannel)\n",
    "### Data processing and loading...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data processing and loading....\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.Resize(size=(128,128)),\n",
    "                                      transforms.RandomVerticalFlip(),\n",
    "                                      transforms.RandomRotation(20),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=normTensor,std=normTensor)])\n",
    "\n",
    "valid_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=(128,128)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomRotation(20),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=normTensor,std=normTensor)\n",
    "    #torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3150normal\n",
      "Found 3150tuberculosis\n"
     ]
    }
   ],
   "source": [
    "train_dirs = {\n",
    "    'normal': root_dir + '/train/normal/',\n",
    "    'tuberculosis': root_dir + '/train/tuberculosis/'\n",
    "}\n",
    "train_dataset=ChestXRayDataset(train_dirs, train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 175normal\n",
      "Found 175tuberculosis\n"
     ]
    }
   ],
   "source": [
    "valid_dirs = {\n",
    "    'normal': root_dir + '/val/normal/',\n",
    "    'tuberculosis': root_dir + '/val/tuberculosis/'\n",
    "}\n",
    "\n",
    "valid_dataset = ChestXRayDataset(valid_dirs, valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 175normal\n",
      "Found 175tuberculosis\n"
     ]
    }
   ],
   "source": [
    "test_dirs = {\n",
    "    'normal': root_dir + '/test/normal/',\n",
    "    'tuberculosis': root_dir + '/test/tuberculosis/'\n",
    "}\n",
    "\n",
    "test_dataset = ChestXRayDataset(test_dirs, valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of training batches 197\n",
      "Num of validation batches 11\n",
      "Num of test batches 11\n"
     ]
    }
   ],
   "source": [
    "dl_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dl_valid = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "dl_test = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "print('Num of training batches', len(dl_train))\n",
    "print('Num of validation batches', len(dl_valid))\n",
    "print('Num of test batches', len(dl_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliaze input dimensions\n",
    "dim = torch.ShortTensor(list(train_dataset[0][0].shape[1:]))\n",
    "nCh = int(train_dataset[0][0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([128, 128], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(nCh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LoTeNet\n"
     ]
    }
   ],
   "source": [
    "# Initialize the models\n",
    "if not args.dense_net:\n",
    "\tprint(\"Using LoTeNet\")\n",
    "\tmodel = loTeNet(input_dim=dim, output_dim=output_dim, \n",
    "\t\t\t\t  nCh=nCh, kernel=kernel,\n",
    "\t\t\t\t  bond_dim=args.bond_dim, feature_dim=feature_dim,\n",
    "\t\t\t\t  adaptive_mode=adaptive_mode, periodic_bc=periodic_bc, virtual_dim=1)\n",
    "else:\n",
    "\tprint(\"Densenet Baseline!\")\n",
    "\tmodel = DenseNet(depth=40, growthRate=12, \n",
    "\t\t\t\t\treduction=0.5,bottleneck=True,nClasses=output_dim)\n",
    "model = loTeNet(input_dim=dim, output_dim=output_dim, \n",
    "\t\t\t\t  nCh=nCh, kernel=kernel,\n",
    "\t\t\t\t  bond_dim=args.bond_dim, feature_dim=feature_dim,\n",
    "\t\t\t\t  adaptive_mode=adaptive_mode, periodic_bc=periodic_bc, virtual_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose loss function and optimizer\n",
    "loss_fun = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, \n",
    "                             weight_decay=args.l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:945255\n",
      "Maximum MPS bond dimension = 5\n",
      "Bond dim: 5\n",
      "Number of parameters:945255\n"
     ]
    }
   ],
   "source": [
    "nParam = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters:%d\"%(nParam))\n",
    "print(f\"Maximum MPS bond dimension = {args.bond_dim}\")\n",
    "\n",
    "print(\"Bond dim: %d\"%(args.bond_dim))\n",
    "print(\"Number of parameters:%d\"%(nParam),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Adam w/ learning rate = 5.0e-04\n",
      "Feature_dim: 2, nCh: 1, B:32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using Adam w/ learning rate = {args.lr:.1e}\")\n",
    "print(\"Feature_dim: %d, nCh: %d, B:%d\"%(feature_dim,nCh,batch_size))\n",
    "\n",
    "model = model.to(device)\n",
    "nValid = len(dl_valid)\n",
    "nTrain = len(dl_train)\n",
    "nTest = len(dl_test)\n",
    "\n",
    "maxAuc = 0\n",
    "minLoss = 1e3\n",
    "convCheck = 5\n",
    "convIter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [5/197], Loss: 0.8039\n",
      "Epoch [1/10], Step [10/197], Loss: 0.8388\n",
      "Epoch [1/10], Step [15/197], Loss: 0.8399\n",
      "Epoch [1/10], Step [20/197], Loss: 0.6653\n",
      "Epoch [1/10], Step [25/197], Loss: 0.6585\n",
      "Epoch [1/10], Step [30/197], Loss: 0.5364\n",
      "Epoch [1/10], Step [35/197], Loss: 0.5199\n",
      "Epoch [1/10], Step [40/197], Loss: 0.5150\n",
      "Epoch [1/10], Step [45/197], Loss: 0.3875\n",
      "Epoch [1/10], Step [50/197], Loss: 0.3956\n",
      "Epoch [1/10], Step [55/197], Loss: 0.5289\n",
      "Epoch [1/10], Step [60/197], Loss: 0.3673\n",
      "Epoch [1/10], Step [65/197], Loss: 0.3732\n",
      "Epoch [1/10], Step [70/197], Loss: 0.2648\n",
      "Epoch [1/10], Step [75/197], Loss: 0.4687\n",
      "Epoch [1/10], Step [80/197], Loss: 0.2769\n",
      "Epoch [1/10], Step [85/197], Loss: 0.4923\n",
      "Epoch [1/10], Step [90/197], Loss: 0.4187\n",
      "Epoch [1/10], Step [95/197], Loss: 0.4244\n",
      "Epoch [1/10], Step [100/197], Loss: 0.3784\n",
      "Epoch [1/10], Step [105/197], Loss: 0.3060\n",
      "Epoch [1/10], Step [110/197], Loss: 0.3663\n",
      "Epoch [1/10], Step [115/197], Loss: 0.3219\n",
      "Epoch [1/10], Step [120/197], Loss: 0.2637\n",
      "Epoch [1/10], Step [125/197], Loss: 0.3619\n",
      "Epoch [1/10], Step [130/197], Loss: 0.3650\n",
      "Epoch [1/10], Step [135/197], Loss: 0.2700\n",
      "Epoch [1/10], Step [140/197], Loss: 0.2331\n",
      "Epoch [1/10], Step [145/197], Loss: 0.4753\n",
      "Epoch [1/10], Step [150/197], Loss: 0.8096\n",
      "Epoch [1/10], Step [155/197], Loss: 0.4332\n",
      "Epoch [1/10], Step [160/197], Loss: 0.3119\n",
      "Epoch [1/10], Step [165/197], Loss: 0.3441\n",
      "Epoch [1/10], Step [170/197], Loss: 0.2586\n",
      "Epoch [1/10], Step [175/197], Loss: 0.0990\n",
      "Epoch [1/10], Step [180/197], Loss: 0.3450\n",
      "Epoch [1/10], Step [185/197], Loss: 0.3549\n",
      "Epoch [1/10], Step [190/197], Loss: 0.2447\n",
      "Epoch [1/10], Step [195/197], Loss: 0.1701\n",
      "New Max: 0.9473\n",
      "Test Set Loss:0.2821\tAuc:0.9551\n",
      "Test Set Loss:0.2821\tAuc:0.9551\n",
      "Epoch [2/10], Step [5/197], Loss: 0.3301\n",
      "Epoch [2/10], Step [10/197], Loss: 0.4118\n",
      "Epoch [2/10], Step [15/197], Loss: 0.2078\n",
      "Epoch [2/10], Step [20/197], Loss: 0.2908\n",
      "Epoch [2/10], Step [25/197], Loss: 0.4016\n",
      "Epoch [2/10], Step [30/197], Loss: 0.4260\n",
      "Epoch [2/10], Step [35/197], Loss: 0.4915\n",
      "Epoch [2/10], Step [40/197], Loss: 0.3182\n",
      "Epoch [2/10], Step [45/197], Loss: 0.2250\n",
      "Epoch [2/10], Step [50/197], Loss: 0.4360\n",
      "Epoch [2/10], Step [55/197], Loss: 0.2959\n",
      "Epoch [2/10], Step [60/197], Loss: 0.3625\n",
      "Epoch [2/10], Step [65/197], Loss: 0.1405\n",
      "Epoch [2/10], Step [70/197], Loss: 0.4453\n",
      "Epoch [2/10], Step [75/197], Loss: 0.2344\n",
      "Epoch [2/10], Step [80/197], Loss: 0.2923\n",
      "Epoch [2/10], Step [85/197], Loss: 0.3956\n",
      "Epoch [2/10], Step [90/197], Loss: 0.1901\n",
      "Epoch [2/10], Step [95/197], Loss: 0.3259\n",
      "Epoch [2/10], Step [100/197], Loss: 0.2997\n",
      "Epoch [2/10], Step [105/197], Loss: 0.2425\n",
      "Epoch [2/10], Step [110/197], Loss: 0.1745\n",
      "Epoch [2/10], Step [115/197], Loss: 0.2125\n",
      "Epoch [2/10], Step [120/197], Loss: 0.2561\n",
      "Epoch [2/10], Step [125/197], Loss: 0.3958\n",
      "Epoch [2/10], Step [130/197], Loss: 0.3087\n",
      "Epoch [2/10], Step [135/197], Loss: 0.4451\n",
      "Epoch [2/10], Step [140/197], Loss: 0.4907\n",
      "Epoch [2/10], Step [145/197], Loss: 0.3601\n",
      "Epoch [2/10], Step [150/197], Loss: 0.3580\n",
      "Epoch [2/10], Step [155/197], Loss: 0.2964\n",
      "Epoch [2/10], Step [160/197], Loss: 0.2776\n",
      "Epoch [2/10], Step [165/197], Loss: 0.2505\n",
      "Epoch [2/10], Step [170/197], Loss: 0.3809\n",
      "Epoch [2/10], Step [175/197], Loss: 0.4143\n",
      "Epoch [2/10], Step [180/197], Loss: 0.3440\n",
      "Epoch [2/10], Step [185/197], Loss: 0.2364\n",
      "Epoch [2/10], Step [190/197], Loss: 0.2482\n",
      "Epoch [2/10], Step [195/197], Loss: 0.3882\n",
      "New Max: 0.9682\n",
      "Test Set Loss:0.3012\tAuc:0.9609\n",
      "Test Set Loss:0.3012\tAuc:0.9609\n",
      "Epoch [3/10], Step [5/197], Loss: 0.1494\n",
      "Epoch [3/10], Step [10/197], Loss: 0.3103\n",
      "Epoch [3/10], Step [15/197], Loss: 0.3057\n",
      "Epoch [3/10], Step [20/197], Loss: 0.1911\n",
      "Epoch [3/10], Step [25/197], Loss: 0.4318\n",
      "Epoch [3/10], Step [30/197], Loss: 0.2790\n",
      "Epoch [3/10], Step [35/197], Loss: 0.1161\n",
      "Epoch [3/10], Step [40/197], Loss: 0.2057\n",
      "Epoch [3/10], Step [45/197], Loss: 0.3891\n",
      "Epoch [3/10], Step [50/197], Loss: 0.1900\n",
      "Epoch [3/10], Step [55/197], Loss: 0.1293\n",
      "Epoch [3/10], Step [60/197], Loss: 0.3538\n",
      "Epoch [3/10], Step [65/197], Loss: 0.2536\n",
      "Epoch [3/10], Step [70/197], Loss: 0.3341\n",
      "Epoch [3/10], Step [75/197], Loss: 0.3368\n",
      "Epoch [3/10], Step [80/197], Loss: 0.2940\n",
      "Epoch [3/10], Step [85/197], Loss: 0.2615\n",
      "Epoch [3/10], Step [90/197], Loss: 0.2253\n",
      "Epoch [3/10], Step [95/197], Loss: 0.1181\n",
      "Epoch [3/10], Step [100/197], Loss: 0.1397\n",
      "Epoch [3/10], Step [105/197], Loss: 0.1498\n",
      "Epoch [3/10], Step [110/197], Loss: 0.2344\n",
      "Epoch [3/10], Step [115/197], Loss: 0.1837\n",
      "Epoch [3/10], Step [120/197], Loss: 0.0762\n",
      "Epoch [3/10], Step [125/197], Loss: 0.2889\n",
      "Epoch [3/10], Step [130/197], Loss: 0.2014\n",
      "Epoch [3/10], Step [135/197], Loss: 0.2345\n",
      "Epoch [3/10], Step [140/197], Loss: 0.3354\n",
      "Epoch [3/10], Step [145/197], Loss: 0.3337\n",
      "Epoch [3/10], Step [150/197], Loss: 0.1444\n",
      "Epoch [3/10], Step [155/197], Loss: 0.2620\n",
      "Epoch [3/10], Step [160/197], Loss: 0.3260\n",
      "Epoch [3/10], Step [165/197], Loss: 0.2420\n",
      "Epoch [3/10], Step [170/197], Loss: 0.2719\n",
      "Epoch [3/10], Step [175/197], Loss: 0.3349\n",
      "Epoch [3/10], Step [180/197], Loss: 0.2668\n",
      "Epoch [3/10], Step [185/197], Loss: 0.1402\n",
      "Epoch [3/10], Step [190/197], Loss: 0.1949\n",
      "Epoch [3/10], Step [195/197], Loss: 0.2651\n",
      "Epoch [4/10], Step [5/197], Loss: 0.2147\n",
      "Epoch [4/10], Step [10/197], Loss: 0.2300\n",
      "Epoch [4/10], Step [15/197], Loss: 0.2126\n",
      "Epoch [4/10], Step [20/197], Loss: 0.2250\n",
      "Epoch [4/10], Step [25/197], Loss: 0.1694\n",
      "Epoch [4/10], Step [30/197], Loss: 0.2338\n",
      "Epoch [4/10], Step [35/197], Loss: 0.1950\n",
      "Epoch [4/10], Step [40/197], Loss: 0.2947\n",
      "Epoch [4/10], Step [45/197], Loss: 0.3999\n",
      "Epoch [4/10], Step [50/197], Loss: 0.3943\n",
      "Epoch [4/10], Step [55/197], Loss: 0.1391\n",
      "Epoch [4/10], Step [60/197], Loss: 0.1633\n",
      "Epoch [4/10], Step [65/197], Loss: 0.2019\n",
      "Epoch [4/10], Step [70/197], Loss: 0.1942\n",
      "Epoch [4/10], Step [75/197], Loss: 0.3178\n",
      "Epoch [4/10], Step [80/197], Loss: 0.1969\n",
      "Epoch [4/10], Step [85/197], Loss: 0.2682\n",
      "Epoch [4/10], Step [90/197], Loss: 0.3513\n",
      "Epoch [4/10], Step [95/197], Loss: 0.2401\n",
      "Epoch [4/10], Step [100/197], Loss: 0.2582\n",
      "Epoch [4/10], Step [105/197], Loss: 0.3628\n",
      "Epoch [4/10], Step [110/197], Loss: 0.3051\n",
      "Epoch [4/10], Step [115/197], Loss: 0.0928\n",
      "Epoch [4/10], Step [120/197], Loss: 0.2518\n",
      "Epoch [4/10], Step [125/197], Loss: 0.1948\n",
      "Epoch [4/10], Step [130/197], Loss: 0.3283\n",
      "Epoch [4/10], Step [135/197], Loss: 0.3408\n",
      "Epoch [4/10], Step [140/197], Loss: 0.2434\n",
      "Epoch [4/10], Step [145/197], Loss: 0.2272\n",
      "Epoch [4/10], Step [150/197], Loss: 0.2815\n",
      "Epoch [4/10], Step [155/197], Loss: 0.3631\n",
      "Epoch [4/10], Step [160/197], Loss: 0.2428\n",
      "Epoch [4/10], Step [165/197], Loss: 0.1613\n",
      "Epoch [4/10], Step [170/197], Loss: 0.1874\n",
      "Epoch [4/10], Step [175/197], Loss: 0.1665\n",
      "Epoch [4/10], Step [180/197], Loss: 0.2317\n",
      "Epoch [4/10], Step [185/197], Loss: 0.0971\n",
      "Epoch [4/10], Step [190/197], Loss: 0.0619\n",
      "Epoch [4/10], Step [195/197], Loss: 0.2327\n",
      "New Max: 0.9703\n",
      "Test Set Loss:0.2671\tAuc:0.9711\n",
      "Test Set Loss:0.2671\tAuc:0.9711\n",
      "Epoch [5/10], Step [5/197], Loss: 0.1688\n",
      "Epoch [5/10], Step [10/197], Loss: 0.2488\n",
      "Epoch [5/10], Step [15/197], Loss: 0.1963\n",
      "Epoch [5/10], Step [20/197], Loss: 0.2732\n",
      "Epoch [5/10], Step [25/197], Loss: 0.3268\n",
      "Epoch [5/10], Step [30/197], Loss: 0.4012\n",
      "Epoch [5/10], Step [35/197], Loss: 0.1277\n",
      "Epoch [5/10], Step [40/197], Loss: 0.1341\n",
      "Epoch [5/10], Step [45/197], Loss: 0.1882\n",
      "Epoch [5/10], Step [50/197], Loss: 0.2916\n",
      "Epoch [5/10], Step [55/197], Loss: 0.2816\n",
      "Epoch [5/10], Step [60/197], Loss: 0.3754\n",
      "Epoch [5/10], Step [65/197], Loss: 0.2075\n",
      "Epoch [5/10], Step [70/197], Loss: 0.2050\n",
      "Epoch [5/10], Step [75/197], Loss: 0.1740\n",
      "Epoch [5/10], Step [80/197], Loss: 0.0915\n",
      "Epoch [5/10], Step [85/197], Loss: 0.1398\n",
      "Epoch [5/10], Step [90/197], Loss: 0.1333\n",
      "Epoch [5/10], Step [95/197], Loss: 0.1457\n",
      "Epoch [5/10], Step [100/197], Loss: 0.0790\n",
      "Epoch [5/10], Step [105/197], Loss: 0.1740\n",
      "Epoch [5/10], Step [110/197], Loss: 0.0599\n",
      "Epoch [5/10], Step [115/197], Loss: 0.4053\n",
      "Epoch [5/10], Step [120/197], Loss: 0.1950\n",
      "Epoch [5/10], Step [125/197], Loss: 0.2771\n",
      "Epoch [5/10], Step [130/197], Loss: 0.2420\n",
      "Epoch [5/10], Step [135/197], Loss: 0.1642\n",
      "Epoch [5/10], Step [140/197], Loss: 0.2132\n",
      "Epoch [5/10], Step [145/197], Loss: 0.2549\n",
      "Epoch [5/10], Step [150/197], Loss: 0.1675\n",
      "Epoch [5/10], Step [155/197], Loss: 0.1434\n",
      "Epoch [5/10], Step [160/197], Loss: 0.2525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [165/197], Loss: 0.2074\n",
      "Epoch [5/10], Step [170/197], Loss: 0.2783\n",
      "Epoch [5/10], Step [175/197], Loss: 0.2849\n",
      "Epoch [5/10], Step [180/197], Loss: 0.3362\n",
      "Epoch [5/10], Step [185/197], Loss: 0.4638\n",
      "Epoch [5/10], Step [190/197], Loss: 0.2619\n",
      "Epoch [5/10], Step [195/197], Loss: 0.3470\n",
      "New Max: 0.9779\n",
      "Test Set Loss:0.2294\tAuc:0.9671\n",
      "Test Set Loss:0.2294\tAuc:0.9671\n",
      "Epoch [6/10], Step [5/197], Loss: 0.4477\n",
      "Epoch [6/10], Step [10/197], Loss: 0.2106\n",
      "Epoch [6/10], Step [15/197], Loss: 0.3326\n",
      "Epoch [6/10], Step [20/197], Loss: 0.3489\n",
      "Epoch [6/10], Step [25/197], Loss: 0.5271\n",
      "Epoch [6/10], Step [30/197], Loss: 0.1158\n",
      "Epoch [6/10], Step [35/197], Loss: 0.1148\n",
      "Epoch [6/10], Step [40/197], Loss: 0.2300\n",
      "Epoch [6/10], Step [45/197], Loss: 0.3432\n",
      "Epoch [6/10], Step [50/197], Loss: 0.2011\n",
      "Epoch [6/10], Step [55/197], Loss: 0.3551\n",
      "Epoch [6/10], Step [60/197], Loss: 0.4555\n",
      "Epoch [6/10], Step [65/197], Loss: 0.1336\n",
      "Epoch [6/10], Step [70/197], Loss: 0.2589\n",
      "Epoch [6/10], Step [75/197], Loss: 0.0533\n",
      "Epoch [6/10], Step [80/197], Loss: 0.3952\n",
      "Epoch [6/10], Step [85/197], Loss: 0.1563\n",
      "Epoch [6/10], Step [90/197], Loss: 0.1604\n",
      "Epoch [6/10], Step [95/197], Loss: 0.1272\n",
      "Epoch [6/10], Step [100/197], Loss: 0.1373\n",
      "Epoch [6/10], Step [105/197], Loss: 0.2192\n",
      "Epoch [6/10], Step [110/197], Loss: 0.2353\n",
      "Epoch [6/10], Step [115/197], Loss: 0.1907\n",
      "Epoch [6/10], Step [120/197], Loss: 0.2808\n",
      "Epoch [6/10], Step [125/197], Loss: 0.2560\n",
      "Epoch [6/10], Step [130/197], Loss: 0.2530\n",
      "Epoch [6/10], Step [135/197], Loss: 0.0693\n",
      "Epoch [6/10], Step [140/197], Loss: 0.2403\n",
      "Epoch [6/10], Step [145/197], Loss: 0.2966\n",
      "Epoch [6/10], Step [150/197], Loss: 0.1492\n",
      "Epoch [6/10], Step [155/197], Loss: 0.2257\n",
      "Epoch [6/10], Step [160/197], Loss: 0.3143\n",
      "Epoch [6/10], Step [165/197], Loss: 0.0893\n",
      "Epoch [6/10], Step [170/197], Loss: 0.1242\n",
      "Epoch [6/10], Step [175/197], Loss: 0.1112\n",
      "Epoch [6/10], Step [180/197], Loss: 0.0816\n",
      "Epoch [6/10], Step [185/197], Loss: 0.2377\n",
      "Epoch [6/10], Step [190/197], Loss: 0.1747\n",
      "Epoch [6/10], Step [195/197], Loss: 0.1064\n",
      "Epoch [7/10], Step [5/197], Loss: 0.3339\n",
      "Epoch [7/10], Step [10/197], Loss: 0.0704\n",
      "Epoch [7/10], Step [15/197], Loss: 0.3171\n",
      "Epoch [7/10], Step [20/197], Loss: 0.2561\n",
      "Epoch [7/10], Step [25/197], Loss: 0.1661\n",
      "Epoch [7/10], Step [30/197], Loss: 0.2040\n",
      "Epoch [7/10], Step [35/197], Loss: 0.2058\n",
      "Epoch [7/10], Step [40/197], Loss: 0.1951\n",
      "Epoch [7/10], Step [45/197], Loss: 0.2308\n",
      "Epoch [7/10], Step [50/197], Loss: 0.2161\n",
      "Epoch [7/10], Step [55/197], Loss: 0.1631\n",
      "Epoch [7/10], Step [60/197], Loss: 0.1160\n",
      "Epoch [7/10], Step [65/197], Loss: 0.1591\n",
      "Epoch [7/10], Step [70/197], Loss: 0.1418\n",
      "Epoch [7/10], Step [75/197], Loss: 0.2444\n",
      "Epoch [7/10], Step [80/197], Loss: 0.2570\n",
      "Epoch [7/10], Step [85/197], Loss: 0.0826\n",
      "Epoch [7/10], Step [90/197], Loss: 0.0350\n",
      "Epoch [7/10], Step [95/197], Loss: 0.1242\n",
      "Epoch [7/10], Step [100/197], Loss: 0.0347\n",
      "Epoch [7/10], Step [105/197], Loss: 0.2687\n",
      "Epoch [7/10], Step [110/197], Loss: 0.1685\n",
      "Epoch [7/10], Step [115/197], Loss: 0.2214\n",
      "Epoch [7/10], Step [120/197], Loss: 0.2055\n",
      "Epoch [7/10], Step [125/197], Loss: 0.1314\n",
      "Epoch [7/10], Step [130/197], Loss: 0.1477\n",
      "Epoch [7/10], Step [135/197], Loss: 0.0914\n",
      "Epoch [7/10], Step [140/197], Loss: 0.0572\n",
      "Epoch [7/10], Step [145/197], Loss: 0.2909\n",
      "Epoch [7/10], Step [150/197], Loss: 0.1774\n",
      "Epoch [7/10], Step [155/197], Loss: 0.2791\n",
      "Epoch [7/10], Step [160/197], Loss: 0.2231\n",
      "Epoch [7/10], Step [165/197], Loss: 0.1606\n",
      "Epoch [7/10], Step [170/197], Loss: 0.3105\n",
      "Epoch [7/10], Step [175/197], Loss: 0.1818\n",
      "Epoch [7/10], Step [180/197], Loss: 0.2332\n",
      "Epoch [7/10], Step [185/197], Loss: 0.0555\n",
      "Epoch [7/10], Step [190/197], Loss: 0.2233\n",
      "Epoch [7/10], Step [195/197], Loss: 0.1419\n",
      "Epoch [8/10], Step [5/197], Loss: 0.2100\n",
      "Epoch [8/10], Step [10/197], Loss: 0.2790\n",
      "Epoch [8/10], Step [15/197], Loss: 0.2597\n",
      "Epoch [8/10], Step [20/197], Loss: 0.0902\n",
      "Epoch [8/10], Step [25/197], Loss: 0.1790\n",
      "Epoch [8/10], Step [30/197], Loss: 0.2194\n",
      "Epoch [8/10], Step [35/197], Loss: 0.2468\n",
      "Epoch [8/10], Step [40/197], Loss: 0.4042\n",
      "Epoch [8/10], Step [45/197], Loss: 0.1901\n",
      "Epoch [8/10], Step [50/197], Loss: 0.1310\n",
      "Epoch [8/10], Step [55/197], Loss: 0.1953\n",
      "Epoch [8/10], Step [60/197], Loss: 0.1625\n",
      "Epoch [8/10], Step [65/197], Loss: 0.1232\n",
      "Epoch [8/10], Step [70/197], Loss: 0.2259\n",
      "Epoch [8/10], Step [75/197], Loss: 0.1140\n",
      "Epoch [8/10], Step [80/197], Loss: 0.1912\n",
      "Epoch [8/10], Step [85/197], Loss: 0.2324\n",
      "Epoch [8/10], Step [90/197], Loss: 0.1678\n",
      "Epoch [8/10], Step [95/197], Loss: 0.2566\n",
      "Epoch [8/10], Step [100/197], Loss: 0.0631\n",
      "Epoch [8/10], Step [105/197], Loss: 0.2346\n",
      "Epoch [8/10], Step [110/197], Loss: 0.2786\n",
      "Epoch [8/10], Step [115/197], Loss: 0.0700\n",
      "Epoch [8/10], Step [120/197], Loss: 0.1091\n",
      "Epoch [8/10], Step [125/197], Loss: 0.2347\n",
      "Epoch [8/10], Step [130/197], Loss: 0.1215\n",
      "Epoch [8/10], Step [135/197], Loss: 0.3633\n",
      "Epoch [8/10], Step [140/197], Loss: 0.2550\n",
      "Epoch [8/10], Step [145/197], Loss: 0.2056\n",
      "Epoch [8/10], Step [150/197], Loss: 0.1823\n",
      "Epoch [8/10], Step [155/197], Loss: 0.0784\n",
      "Epoch [8/10], Step [160/197], Loss: 0.0868\n",
      "Epoch [8/10], Step [165/197], Loss: 0.2241\n",
      "Epoch [8/10], Step [170/197], Loss: 0.1012\n",
      "Epoch [8/10], Step [175/197], Loss: 0.1122\n",
      "Epoch [8/10], Step [180/197], Loss: 0.1775\n",
      "Epoch [8/10], Step [185/197], Loss: 0.1695\n",
      "Epoch [8/10], Step [190/197], Loss: 0.2254\n",
      "Epoch [8/10], Step [195/197], Loss: 0.2271\n",
      "Epoch [9/10], Step [5/197], Loss: 0.1929\n",
      "Epoch [9/10], Step [10/197], Loss: 0.0685\n",
      "Epoch [9/10], Step [15/197], Loss: 0.3295\n",
      "Epoch [9/10], Step [20/197], Loss: 0.4415\n",
      "Epoch [9/10], Step [25/197], Loss: 0.1843\n",
      "Epoch [9/10], Step [30/197], Loss: 0.3016\n",
      "Epoch [9/10], Step [35/197], Loss: 0.1936\n",
      "Epoch [9/10], Step [40/197], Loss: 0.2783\n",
      "Epoch [9/10], Step [45/197], Loss: 0.1742\n",
      "Epoch [9/10], Step [50/197], Loss: 0.1773\n",
      "Epoch [9/10], Step [55/197], Loss: 0.1865\n",
      "Epoch [9/10], Step [60/197], Loss: 0.0870\n",
      "Epoch [9/10], Step [65/197], Loss: 0.0919\n",
      "Epoch [9/10], Step [70/197], Loss: 0.1045\n",
      "Epoch [9/10], Step [75/197], Loss: 0.0622\n",
      "Epoch [9/10], Step [80/197], Loss: 0.1510\n",
      "Epoch [9/10], Step [85/197], Loss: 0.1640\n",
      "Epoch [9/10], Step [90/197], Loss: 0.0877\n",
      "Epoch [9/10], Step [95/197], Loss: 0.1710\n",
      "Epoch [9/10], Step [100/197], Loss: 0.1433\n",
      "Epoch [9/10], Step [105/197], Loss: 0.1826\n",
      "Epoch [9/10], Step [110/197], Loss: 0.1969\n",
      "Epoch [9/10], Step [115/197], Loss: 0.3483\n",
      "Epoch [9/10], Step [120/197], Loss: 0.0969\n",
      "Epoch [9/10], Step [125/197], Loss: 0.2750\n",
      "Epoch [9/10], Step [130/197], Loss: 0.1179\n",
      "Epoch [9/10], Step [135/197], Loss: 0.0455\n",
      "Epoch [9/10], Step [140/197], Loss: 0.1024\n",
      "Epoch [9/10], Step [145/197], Loss: 0.3043\n",
      "Epoch [9/10], Step [150/197], Loss: 0.2196\n",
      "Epoch [9/10], Step [155/197], Loss: 0.0888\n",
      "Epoch [9/10], Step [160/197], Loss: 0.2353\n",
      "Epoch [9/10], Step [165/197], Loss: 0.2132\n",
      "Epoch [9/10], Step [170/197], Loss: 0.1266\n",
      "Epoch [9/10], Step [175/197], Loss: 0.2535\n",
      "Epoch [9/10], Step [180/197], Loss: 0.2036\n",
      "Epoch [9/10], Step [185/197], Loss: 0.2073\n",
      "Epoch [9/10], Step [190/197], Loss: 0.2788\n",
      "Epoch [9/10], Step [195/197], Loss: 0.0375\n",
      "New Max: 0.9791\n",
      "Test Set Loss:0.1963\tAuc:0.9791\n",
      "Test Set Loss:0.1963\tAuc:0.9791\n",
      "Epoch [10/10], Step [5/197], Loss: 0.1811\n",
      "Epoch [10/10], Step [10/197], Loss: 0.1847\n",
      "Epoch [10/10], Step [15/197], Loss: 0.1151\n",
      "Epoch [10/10], Step [20/197], Loss: 0.2898\n",
      "Epoch [10/10], Step [25/197], Loss: 0.0984\n",
      "Epoch [10/10], Step [30/197], Loss: 0.1136\n",
      "Epoch [10/10], Step [35/197], Loss: 0.2092\n",
      "Epoch [10/10], Step [40/197], Loss: 0.1790\n",
      "Epoch [10/10], Step [45/197], Loss: 0.3154\n",
      "Epoch [10/10], Step [50/197], Loss: 0.2213\n",
      "Epoch [10/10], Step [55/197], Loss: 0.1815\n",
      "Epoch [10/10], Step [60/197], Loss: 0.0452\n",
      "Epoch [10/10], Step [65/197], Loss: 0.1130\n",
      "Epoch [10/10], Step [70/197], Loss: 0.2431\n",
      "Epoch [10/10], Step [75/197], Loss: 0.1528\n",
      "Epoch [10/10], Step [80/197], Loss: 0.0988\n",
      "Epoch [10/10], Step [85/197], Loss: 0.1895\n",
      "Epoch [10/10], Step [90/197], Loss: 0.0751\n",
      "Epoch [10/10], Step [95/197], Loss: 0.1748\n",
      "Epoch [10/10], Step [100/197], Loss: 0.1710\n",
      "Epoch [10/10], Step [105/197], Loss: 0.0647\n",
      "Epoch [10/10], Step [110/197], Loss: 0.0253\n",
      "Epoch [10/10], Step [115/197], Loss: 0.1786\n",
      "Epoch [10/10], Step [120/197], Loss: 0.0726\n",
      "Epoch [10/10], Step [125/197], Loss: 0.1527\n",
      "Epoch [10/10], Step [130/197], Loss: 0.1925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [135/197], Loss: 0.2192\n",
      "Epoch [10/10], Step [140/197], Loss: 0.1493\n",
      "Epoch [10/10], Step [145/197], Loss: 0.0851\n",
      "Epoch [10/10], Step [150/197], Loss: 0.2100\n",
      "Epoch [10/10], Step [155/197], Loss: 0.2177\n",
      "Epoch [10/10], Step [160/197], Loss: 0.0948\n",
      "Epoch [10/10], Step [165/197], Loss: 0.1352\n",
      "Epoch [10/10], Step [170/197], Loss: 0.0500\n",
      "Epoch [10/10], Step [175/197], Loss: 0.2344\n",
      "Epoch [10/10], Step [180/197], Loss: 0.1593\n",
      "Epoch [10/10], Step [185/197], Loss: 0.2021\n",
      "Epoch [10/10], Step [190/197], Loss: 0.2100\n",
      "Epoch [10/10], Step [195/197], Loss: 0.1376\n",
      "New Max: 0.9864\n",
      "Test Set Loss:0.1821\tAuc:0.9802\n",
      "Test Set Loss:0.1821\tAuc:0.9802\n"
     ]
    }
   ],
   "source": [
    "# Let's start training!\n",
    "for epoch in range(args.num_epochs):\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    #t = time.time()\n",
    "    model.train()\n",
    "    predsNp = np.zeros(1)\n",
    "    labelsNp = np.zeros(1)\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(dl_train):\n",
    "        \n",
    "        # convert inputs and labels  and scores to float tensor\n",
    "        inputs = inputs.to(device).type(dtype=torch.float)\n",
    "        labels = labels.to(device).type(dtype=torch.float)\n",
    "        labelsNp = np.concatenate((labelsNp, labels.cpu().numpy()))\n",
    "\n",
    "        scores = torch.sigmoid(model(inputs)).type(dtype=torch.float)\n",
    "\n",
    "        preds = scores\n",
    "        loss = loss_fun(scores, labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predsNp = np.concatenate((predsNp, preds.detach().cpu().numpy()))\n",
    "            running_loss += loss\n",
    "\n",
    "        # Backpropagate and update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 5 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, args.num_epochs, i+1, nTrain, loss.item()))\n",
    "\n",
    "    accuracy = computeAuc(labelsNp,predsNp)\n",
    "\n",
    "    # Evaluate on Validation set \n",
    "    with torch.no_grad():\n",
    "\n",
    "        vl_acc, vl_loss = evaluate(dl_valid)\n",
    "        if vl_acc > maxAuc or vl_loss < minLoss:\n",
    "            if vl_loss < minLoss:\n",
    "                minLoss = vl_loss\n",
    "            if vl_acc > maxAuc:\n",
    "                ### Predict on test set\n",
    "                ts_acc, ts_loss = evaluate(dl_test)\n",
    "                maxAuc = vl_acc\n",
    "                print('New Max: %.4f'%maxAuc)\n",
    "                print('Test Set Loss:%.4f\tAuc:%.4f'%(ts_loss, ts_acc))\n",
    "                print('Test Set Loss:%.4f\tAuc:%.4f'%(ts_loss, ts_acc))\n",
    "            convEpoch = epoch\n",
    "            convIter = 0\n",
    "        else:\n",
    "            convIter += 1\n",
    "        if convIter == convCheck:\n",
    "            if not args.dense_net:\n",
    "                print(\"MPS\")\n",
    "            else:\n",
    "                print(\"DenseNet\")\n",
    "            print(\"Converged at epoch:%d with AUC:%.4f\"%(convEpoch+1,maxAuc))\n",
    "\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
