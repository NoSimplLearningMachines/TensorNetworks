{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TB detection using Tensor Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version 1.3.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from models.lotenet import loTeNet\n",
    "from torchvision import transforms, datasets\n",
    "import pdb\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from models.Densenet import *\n",
    "from utils.tools import *\n",
    "import argparse\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print('Using PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['normal', 'pneumonia']\n",
    "root_dir = '/home/mashjunior/loTeNet_pytorch/TBChestXRays/chest_xray/chest_xray'\n",
    "source_dirs = ['normal' ,'pneumonia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXRayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dirs,transform):\n",
    "        def get_images(class_name):\n",
    "            images = [x for x in os.listdir(image_dirs[class_name]) if x.lower().endswith('jpeg')]\n",
    "            print(f'Found {len(images)}{class_name}')\n",
    "            return images\n",
    "        self.images={}\n",
    "        self.class_names=['normal','pneumonia']\n",
    "        for c in self.class_names:\n",
    "            self.images[c]=get_images(c)\n",
    "        self.image_dirs=image_dirs\n",
    "        self.transform=transform\n",
    "    def __len__(self):\n",
    "        return sum([len(self.images[c]) for c in self.class_names])\n",
    "    def __getitem__(self, index):\n",
    "        class_name=random.choice(self.class_names)\n",
    "        index=index%len(self.images[class_name])\n",
    "        image_name=self.images[class_name][index]\n",
    "        image_path =os.path.join(self.image_dirs[class_name], image_name)\n",
    "        image=Image.open(image_path).convert('L')\n",
    "        return self.transform(image), self.class_names.index(class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globally load device identifier\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "     ### Evaluation funcntion for validation/testing\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vl_acc = 0.\n",
    "        vl_loss = 0.\n",
    "        labelsNp = np.zeros(1)\n",
    "        predsNp = np.zeros(1)\n",
    "        model.eval()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(loader):\n",
    "\n",
    "            inputs = inputs.to(device).type(dtype=torch.float)\n",
    "            labels = labels.to(device).type(dtype=torch.float)\n",
    "            labelsNp = np.concatenate((labelsNp, labels.cpu().numpy()))\n",
    "\n",
    "            # Inference\n",
    "            scores = torch.sigmoid(model(inputs)).type(dtype=torch.float)\n",
    "\n",
    "            preds = scores\n",
    "            loss = loss_fun(scores, labels)\n",
    "            predsNp = np.concatenate((predsNp, preds.cpu().numpy()))\n",
    "            vl_loss += loss.item()\n",
    "\n",
    "        # Compute AUC over the full (valid/test) set\n",
    "        vl_acc = computeAuc(labelsNp[1:],predsNp[1:])\n",
    "        vl_loss = vl_loss/len(loader)\n",
    "\n",
    "    return vl_acc, vl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellaneous initialization\n",
    "torch.manual_seed(1)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--dense_net'], dest='dense_net', nargs=0, const=True, default=False, type=None, choices=None, help='Using Dense Net model', metavar=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument('--num_epochs', type=int, default=5, help='Number of training epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n",
    "parser.add_argument('--lr', type=float, default=5e-4, help='Learning rate')\n",
    "parser.add_argument('--l2', type=float, default=0, help='L2 regularisation')\n",
    "parser.add_argument('--aug', action='store_true', default=False, help='Use data augmentation')\n",
    "parser.add_argument('--data_path', type=str, default=root_dir,help='Path to data.')\n",
    "parser.add_argument('--bond_dim', type=int, default=5, help='MPS Bond dimension')\n",
    "parser.add_argument('--nChannel', type=int, default=1, help='Number of input channels')\n",
    "parser.add_argument('--dense_net', action='store_true', default=False, help='Using Dense Net model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = args.batch_size\n",
    "\n",
    "# LoTeNet parameters\n",
    "adaptive_mode = False \n",
    "periodic_bc   = False\n",
    "\n",
    "kernel = 2 # Stride along spatial dimensions\n",
    "output_dim = 1 # output dimension\n",
    " \n",
    "feature_dim = 2\n",
    "\n",
    "#logFile = time.strftime(\"%Y%m%d_%H_%M\")+'.txt'\n",
    "#makeLogFile(logFile)\n",
    "\n",
    "normTensor = 0.5*torch.ones(args.nChannel)\n",
    "### Data processing and loading...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data processing and loading....\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.Resize(size=(128,128)),\n",
    "                                      transforms.RandomVerticalFlip(),\n",
    "                                      transforms.RandomRotation(20),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=normTensor,std=normTensor)])\n",
    "\n",
    "valid_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=(128,128)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomRotation(20),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=normTensor,std=normTensor)\n",
    "    #torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1341normal\n",
      "Found 3875pneumonia\n"
     ]
    }
   ],
   "source": [
    "train_dirs = {\n",
    "    'normal': root_dir + '/train/normal',\n",
    "    'pneumonia': root_dir + '/train/pneumonia'\n",
    "}\n",
    "train_dataset=ChestXRayDataset(train_dirs, train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 234normal\n",
      "Found 390pneumonia\n"
     ]
    }
   ],
   "source": [
    "valid_dirs = {\n",
    "    'normal': root_dir + '/val/normal',\n",
    "    'pneumonia': root_dir + '/val/pneumonia'\n",
    "}\n",
    "\n",
    "valid_dataset = ChestXRayDataset(valid_dirs, valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8normal\n",
      "Found 8pneumonia\n"
     ]
    }
   ],
   "source": [
    "test_dirs = {\n",
    "    'normal': root_dir + '/test/normal',\n",
    "    'pneumonia': root_dir + '/test/pneumonia'\n",
    "}\n",
    "\n",
    "test_dataset = ChestXRayDataset(test_dirs, valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of training batches 163\n",
      "Num of validation batches 20\n",
      "Num of test batches 1\n"
     ]
    }
   ],
   "source": [
    "dl_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dl_valid = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "dl_test = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "print('Num of training batches', len(dl_train))\n",
    "print('Num of validation batches', len(dl_valid))\n",
    "print('Num of test batches', len(dl_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliaze input dimensions\n",
    "dim = torch.ShortTensor(list(train_dataset[0][0].shape[1:]))\n",
    "nCh = int(train_dataset[0][0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([128, 128], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(nCh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LoTeNet\n"
     ]
    }
   ],
   "source": [
    "# Initialize the models\n",
    "if not args.dense_net:\n",
    "\tprint(\"Using LoTeNet\")\n",
    "\tmodel = loTeNet(input_dim=dim, output_dim=output_dim, \n",
    "\t\t\t\t  nCh=nCh, kernel=kernel,\n",
    "\t\t\t\t  bond_dim=args.bond_dim, feature_dim=feature_dim,\n",
    "\t\t\t\t  adaptive_mode=adaptive_mode, periodic_bc=periodic_bc, virtual_dim=1)\n",
    "else:\n",
    "\tprint(\"Densenet Baseline!\")\n",
    "\tmodel = DenseNet(depth=40, growthRate=12, \n",
    "\t\t\t\t\treduction=0.5,bottleneck=True,nClasses=output_dim)\n",
    "model = loTeNet(input_dim=dim, output_dim=output_dim, \n",
    "\t\t\t\t  nCh=nCh, kernel=kernel,\n",
    "\t\t\t\t  bond_dim=args.bond_dim, feature_dim=feature_dim,\n",
    "\t\t\t\t  adaptive_mode=adaptive_mode, periodic_bc=periodic_bc, virtual_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose loss function and optimizer\n",
    "loss_fun = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, \n",
    "                             weight_decay=args.l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:945255\n",
      "Maximum MPS bond dimension = 5\n",
      "Bond dim: 5\n",
      "Number of parameters:945255\n"
     ]
    }
   ],
   "source": [
    "nParam = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters:%d\"%(nParam))\n",
    "print(f\"Maximum MPS bond dimension = {args.bond_dim}\")\n",
    "\n",
    "print(\"Bond dim: %d\"%(args.bond_dim))\n",
    "print(\"Number of parameters:%d\"%(nParam),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Adam w/ learning rate = 5.0e-04\n",
      "Feature_dim: 2, nCh: 1, B:32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using Adam w/ learning rate = {args.lr:.1e}\")\n",
    "print(\"Feature_dim: %d, nCh: %d, B:%d\"%(feature_dim,nCh,batch_size))\n",
    "\n",
    "model = model.to(device)\n",
    "nValid = len(dl_valid)\n",
    "nTrain = len(dl_train)\n",
    "nTest = len(dl_test)\n",
    "\n",
    "maxAuc = 0\n",
    "minLoss = 1e3\n",
    "convCheck = 5\n",
    "convIter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5/163], Loss: 0.7428\n",
      "Epoch [1/5], Step [10/163], Loss: 0.7542\n",
      "Epoch [1/5], Step [15/163], Loss: 0.6743\n",
      "Epoch [1/5], Step [20/163], Loss: 0.6873\n",
      "Epoch [1/5], Step [25/163], Loss: 0.5684\n",
      "Epoch [1/5], Step [30/163], Loss: 0.5836\n",
      "Epoch [1/5], Step [35/163], Loss: 0.5208\n",
      "Epoch [1/5], Step [40/163], Loss: 0.4957\n",
      "Epoch [1/5], Step [45/163], Loss: 0.4612\n",
      "Epoch [1/5], Step [50/163], Loss: 0.3088\n",
      "Epoch [1/5], Step [55/163], Loss: 0.3481\n",
      "Epoch [1/5], Step [60/163], Loss: 0.3933\n",
      "Epoch [1/5], Step [65/163], Loss: 0.1834\n",
      "Epoch [1/5], Step [70/163], Loss: 0.4002\n",
      "Epoch [1/5], Step [75/163], Loss: 0.2036\n",
      "Epoch [1/5], Step [80/163], Loss: 0.3790\n",
      "Epoch [1/5], Step [85/163], Loss: 0.3848\n",
      "Epoch [1/5], Step [90/163], Loss: 0.5487\n",
      "Epoch [1/5], Step [95/163], Loss: 0.4169\n",
      "Epoch [1/5], Step [100/163], Loss: 0.5524\n",
      "Epoch [1/5], Step [105/163], Loss: 0.2403\n",
      "Epoch [1/5], Step [110/163], Loss: 0.4750\n",
      "Epoch [1/5], Step [115/163], Loss: 0.3068\n",
      "Epoch [1/5], Step [120/163], Loss: 0.3765\n",
      "Epoch [1/5], Step [125/163], Loss: 0.1883\n",
      "Epoch [1/5], Step [130/163], Loss: 0.3456\n",
      "Epoch [1/5], Step [135/163], Loss: 0.4371\n",
      "Epoch [1/5], Step [140/163], Loss: 0.4729\n",
      "Epoch [1/5], Step [145/163], Loss: 0.3736\n",
      "Epoch [1/5], Step [150/163], Loss: 0.3556\n",
      "Epoch [1/5], Step [155/163], Loss: 0.2853\n",
      "Epoch [1/5], Step [160/163], Loss: 0.5168\n",
      "New Max: 0.8718\n",
      "Test Set Loss:0.8367\tAuc:0.6667\n",
      "Test Set Loss:0.8367\tAuc:0.6667\n",
      "Epoch [2/5], Step [5/163], Loss: 0.2322\n",
      "Epoch [2/5], Step [10/163], Loss: 0.2135\n",
      "Epoch [2/5], Step [15/163], Loss: 0.2124\n",
      "Epoch [2/5], Step [20/163], Loss: 0.1453\n",
      "Epoch [2/5], Step [25/163], Loss: 0.2151\n",
      "Epoch [2/5], Step [30/163], Loss: 0.6630\n",
      "Epoch [2/5], Step [35/163], Loss: 0.4159\n",
      "Epoch [2/5], Step [40/163], Loss: 0.1701\n",
      "Epoch [2/5], Step [45/163], Loss: 0.3056\n",
      "Epoch [2/5], Step [50/163], Loss: 0.4273\n",
      "Epoch [2/5], Step [55/163], Loss: 0.7527\n",
      "Epoch [2/5], Step [60/163], Loss: 0.3218\n",
      "Epoch [2/5], Step [65/163], Loss: 0.2104\n",
      "Epoch [2/5], Step [70/163], Loss: 0.2293\n",
      "Epoch [2/5], Step [75/163], Loss: 0.2678\n",
      "Epoch [2/5], Step [80/163], Loss: 0.3234\n",
      "Epoch [2/5], Step [85/163], Loss: 0.2990\n",
      "Epoch [2/5], Step [90/163], Loss: 0.2540\n",
      "Epoch [2/5], Step [95/163], Loss: 0.3097\n",
      "Epoch [2/5], Step [100/163], Loss: 0.3264\n",
      "Epoch [2/5], Step [105/163], Loss: 0.1528\n",
      "Epoch [2/5], Step [110/163], Loss: 0.5149\n",
      "Epoch [2/5], Step [115/163], Loss: 0.2476\n",
      "Epoch [2/5], Step [120/163], Loss: 0.4067\n",
      "Epoch [2/5], Step [125/163], Loss: 0.3474\n",
      "Epoch [2/5], Step [130/163], Loss: 0.2312\n",
      "Epoch [2/5], Step [135/163], Loss: 0.2063\n",
      "Epoch [2/5], Step [140/163], Loss: 0.3050\n",
      "Epoch [2/5], Step [145/163], Loss: 0.1958\n",
      "Epoch [2/5], Step [150/163], Loss: 0.3966\n",
      "Epoch [2/5], Step [155/163], Loss: 0.1150\n",
      "Epoch [2/5], Step [160/163], Loss: 0.3449\n",
      "New Max: 0.9013\n",
      "Test Set Loss:0.4434\tAuc:0.8571\n",
      "Test Set Loss:0.4434\tAuc:0.8571\n",
      "Epoch [3/5], Step [5/163], Loss: 0.5596\n",
      "Epoch [3/5], Step [10/163], Loss: 0.2959\n",
      "Epoch [3/5], Step [15/163], Loss: 0.1760\n",
      "Epoch [3/5], Step [20/163], Loss: 0.3047\n",
      "Epoch [3/5], Step [25/163], Loss: 0.4211\n",
      "Epoch [3/5], Step [30/163], Loss: 0.2172\n",
      "Epoch [3/5], Step [35/163], Loss: 0.2786\n",
      "Epoch [3/5], Step [40/163], Loss: 0.3786\n",
      "Epoch [3/5], Step [45/163], Loss: 0.2163\n",
      "Epoch [3/5], Step [50/163], Loss: 0.3184\n",
      "Epoch [3/5], Step [55/163], Loss: 0.1266\n",
      "Epoch [3/5], Step [60/163], Loss: 0.1584\n",
      "Epoch [3/5], Step [65/163], Loss: 0.2088\n",
      "Epoch [3/5], Step [70/163], Loss: 0.3322\n",
      "Epoch [3/5], Step [75/163], Loss: 0.2273\n",
      "Epoch [3/5], Step [80/163], Loss: 0.1811\n",
      "Epoch [3/5], Step [85/163], Loss: 0.2128\n",
      "Epoch [3/5], Step [90/163], Loss: 0.1923\n",
      "Epoch [3/5], Step [95/163], Loss: 0.1748\n",
      "Epoch [3/5], Step [100/163], Loss: 0.1040\n",
      "Epoch [3/5], Step [105/163], Loss: 0.2275\n",
      "Epoch [3/5], Step [110/163], Loss: 0.5786\n",
      "Epoch [3/5], Step [115/163], Loss: 0.1717\n",
      "Epoch [3/5], Step [120/163], Loss: 0.1719\n",
      "Epoch [3/5], Step [125/163], Loss: 0.2451\n",
      "Epoch [3/5], Step [130/163], Loss: 0.2259\n",
      "Epoch [3/5], Step [135/163], Loss: 0.2224\n",
      "Epoch [3/5], Step [140/163], Loss: 0.2306\n",
      "Epoch [3/5], Step [145/163], Loss: 0.1747\n",
      "Epoch [3/5], Step [150/163], Loss: 0.3002\n",
      "Epoch [3/5], Step [155/163], Loss: 0.3840\n",
      "Epoch [3/5], Step [160/163], Loss: 0.2339\n",
      "Epoch [4/5], Step [5/163], Loss: 0.2059\n",
      "Epoch [4/5], Step [10/163], Loss: 0.1647\n",
      "Epoch [4/5], Step [15/163], Loss: 0.1783\n",
      "Epoch [4/5], Step [20/163], Loss: 0.3580\n",
      "Epoch [4/5], Step [25/163], Loss: 0.2690\n",
      "Epoch [4/5], Step [30/163], Loss: 0.2757\n",
      "Epoch [4/5], Step [35/163], Loss: 0.2065\n",
      "Epoch [4/5], Step [40/163], Loss: 0.1961\n",
      "Epoch [4/5], Step [45/163], Loss: 0.4927\n",
      "Epoch [4/5], Step [50/163], Loss: 0.2133\n",
      "Epoch [4/5], Step [55/163], Loss: 0.1948\n",
      "Epoch [4/5], Step [60/163], Loss: 0.3348\n",
      "Epoch [4/5], Step [65/163], Loss: 0.4079\n",
      "Epoch [4/5], Step [70/163], Loss: 0.3373\n",
      "Epoch [4/5], Step [75/163], Loss: 0.1060\n",
      "Epoch [4/5], Step [80/163], Loss: 0.2126\n",
      "Epoch [4/5], Step [85/163], Loss: 0.1698\n",
      "Epoch [4/5], Step [90/163], Loss: 0.2078\n",
      "Epoch [4/5], Step [95/163], Loss: 0.1730\n",
      "Epoch [4/5], Step [100/163], Loss: 0.4453\n",
      "Epoch [4/5], Step [105/163], Loss: 0.3056\n",
      "Epoch [4/5], Step [110/163], Loss: 0.3647\n",
      "Epoch [4/5], Step [115/163], Loss: 0.2960\n",
      "Epoch [4/5], Step [120/163], Loss: 0.2943\n",
      "Epoch [4/5], Step [125/163], Loss: 0.1377\n",
      "Epoch [4/5], Step [130/163], Loss: 0.1585\n",
      "Epoch [4/5], Step [135/163], Loss: 0.1346\n",
      "Epoch [4/5], Step [140/163], Loss: 0.1253\n",
      "Epoch [4/5], Step [145/163], Loss: 0.0909\n",
      "Epoch [4/5], Step [150/163], Loss: 0.1448\n",
      "Epoch [4/5], Step [155/163], Loss: 0.1951\n",
      "Epoch [4/5], Step [160/163], Loss: 0.2032\n",
      "Epoch [5/5], Step [5/163], Loss: 0.1518\n",
      "Epoch [5/5], Step [10/163], Loss: 0.1848\n",
      "Epoch [5/5], Step [15/163], Loss: 0.3296\n",
      "Epoch [5/5], Step [20/163], Loss: 0.1665\n",
      "Epoch [5/5], Step [25/163], Loss: 0.2002\n",
      "Epoch [5/5], Step [30/163], Loss: 0.2652\n",
      "Epoch [5/5], Step [35/163], Loss: 0.2289\n",
      "Epoch [5/5], Step [40/163], Loss: 0.2044\n",
      "Epoch [5/5], Step [45/163], Loss: 0.2557\n",
      "Epoch [5/5], Step [50/163], Loss: 0.1201\n",
      "Epoch [5/5], Step [55/163], Loss: 0.4038\n",
      "Epoch [5/5], Step [60/163], Loss: 0.2958\n",
      "Epoch [5/5], Step [65/163], Loss: 0.3468\n",
      "Epoch [5/5], Step [70/163], Loss: 0.2273\n",
      "Epoch [5/5], Step [75/163], Loss: 0.2666\n",
      "Epoch [5/5], Step [80/163], Loss: 0.0941\n",
      "Epoch [5/5], Step [85/163], Loss: 0.1970\n",
      "Epoch [5/5], Step [90/163], Loss: 0.0935\n",
      "Epoch [5/5], Step [95/163], Loss: 0.3666\n",
      "Epoch [5/5], Step [100/163], Loss: 0.1663\n",
      "Epoch [5/5], Step [105/163], Loss: 0.1786\n",
      "Epoch [5/5], Step [110/163], Loss: 0.3606\n",
      "Epoch [5/5], Step [115/163], Loss: 0.3166\n",
      "Epoch [5/5], Step [120/163], Loss: 0.2605\n",
      "Epoch [5/5], Step [125/163], Loss: 0.2107\n",
      "Epoch [5/5], Step [130/163], Loss: 0.2371\n",
      "Epoch [5/5], Step [135/163], Loss: 0.2682\n",
      "Epoch [5/5], Step [140/163], Loss: 0.4243\n",
      "Epoch [5/5], Step [145/163], Loss: 0.2845\n",
      "Epoch [5/5], Step [150/163], Loss: 0.1421\n",
      "Epoch [5/5], Step [155/163], Loss: 0.0654\n",
      "Epoch [5/5], Step [160/163], Loss: 0.1964\n",
      "New Max: 0.9081\n",
      "Test Set Loss:0.2680\tAuc:0.9818\n",
      "Test Set Loss:0.2680\tAuc:0.9818\n"
     ]
    }
   ],
   "source": [
    "# Let's start training!\n",
    "for epoch in range(args.num_epochs):\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    #t = time.time()\n",
    "    model.train()\n",
    "    predsNp = np.zeros(1)\n",
    "    labelsNp = np.zeros(1)\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(dl_train):\n",
    "        \n",
    "        # convert inputs and labels  and scores to float tensor\n",
    "        inputs = inputs.to(device).type(dtype=torch.float)\n",
    "        labels = labels.to(device).type(dtype=torch.float)\n",
    "        labelsNp = np.concatenate((labelsNp, labels.cpu().numpy()))\n",
    "\n",
    "        scores = torch.sigmoid(model(inputs)).type(dtype=torch.float)\n",
    "\n",
    "        preds = scores\n",
    "        loss = loss_fun(scores, labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predsNp = np.concatenate((predsNp, preds.detach().cpu().numpy()))\n",
    "            running_loss += loss\n",
    "\n",
    "        # Backpropagate and update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 5 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, args.num_epochs, i+1, nTrain, loss.item()))\n",
    "\n",
    "    accuracy = computeAuc(labelsNp,predsNp)\n",
    "\n",
    "    # Evaluate on Validation set \n",
    "    with torch.no_grad():\n",
    "\n",
    "        vl_acc, vl_loss = evaluate(dl_valid)\n",
    "        if vl_acc > maxAuc or vl_loss < minLoss:\n",
    "            if vl_loss < minLoss:\n",
    "                minLoss = vl_loss\n",
    "            if vl_acc > maxAuc:\n",
    "                ### Predict on test set\n",
    "                ts_acc, ts_loss = evaluate(dl_test)\n",
    "                maxAuc = vl_acc\n",
    "                print('New Max: %.4f'%maxAuc)\n",
    "                print('Test Set Loss:%.4f\tAuc:%.4f'%(ts_loss, ts_acc))\n",
    "                print('Test Set Loss:%.4f\tAuc:%.4f'%(ts_loss, ts_acc))\n",
    "            convEpoch = epoch\n",
    "            convIter = 0\n",
    "        else:\n",
    "            convIter += 1\n",
    "        if convIter == convCheck:\n",
    "            if not args.dense_net:\n",
    "                print(\"MPS\")\n",
    "            else:\n",
    "                print(\"DenseNet\")\n",
    "            print(\"Converged at epoch:%d with AUC:%.4f\"%(convEpoch+1,maxAuc))\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
