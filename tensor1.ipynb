{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import time\n",
    "import torch\n",
    "from models.lotenet import loTeNet\n",
    "from torchvision import transforms, datasets\n",
    "import pdb\n",
    "from utils.lidc_dataset import LIDC\n",
    "from utils.tools import *\n",
    "from models.Densenet import *\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globally load device identifier\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def evaluate(loader):\n",
    "\t### Evaluation funcntion for validation/testing\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tvl_acc = 0.\n",
    "\t\tvl_loss = 0.\n",
    "\t\tlabelsNp = np.zeros(1)\n",
    "\t\tpredsNp = np.zeros(1)\n",
    "\t\tmodel.eval()\n",
    "\n",
    "\t\tfor i, (inputs, labels) in enumerate(loader):\n",
    "\n",
    "\t\t\tinputs = inputs.to(device)\n",
    "\t\t\tlabels = labels.to(device)\n",
    "\t\t\tlabelsNp = np.concatenate((labelsNp, labels.cpu().numpy()))\n",
    "\n",
    "\t\t\t# Inference\n",
    "\t\t\tscores = torch.sigmoid(model(inputs))\n",
    "\n",
    "\t\t\tpreds = scores\n",
    "\t\t\tloss = loss_fun(scores, labels)\n",
    "\t\t\tpredsNp = np.concatenate((predsNp, preds.cpu().numpy()))\n",
    "\t\t\tvl_loss += loss.item()\n",
    "\n",
    "\t\t# Compute AUC over the full (valid/test) set\n",
    "\t\tvl_acc = computeAuc(labelsNp[1:],predsNp[1:])\n",
    "\t\tvl_loss = vl_loss/len(loader)\n",
    "\n",
    "\treturn vl_acc, vl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellaneous initialization\n",
    "torch.manual_seed(1)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num_epochs', type=int, default=5, help='Number of training epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=512, help='Batch size')\n",
    "parser.add_argument('--lr', type=float, default=5e-4, help='Learning rate')\n",
    "parser.add_argument('--l2', type=float, default=0, help='L2 regularisation')\n",
    "parser.add_argument('--aug', action='store_true', default=False, help='Use data augmentation')\n",
    "parser.add_argument('--data_path', type=str, default='lidc/',help='Path to data.')\n",
    "parser.add_argument('--bond_dim', type=int, default=5, help='MPS Bond dimension')\n",
    "parser.add_argument('--nChannel', type=int, default=1, help='Number of input channels')\n",
    "parser.add_argument('--dense_net', action='store_true', default=False, help='Using Dense Net model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = args.batch_size\n",
    "\n",
    "# LoTeNet parameters\n",
    "adaptive_mode = False \n",
    "periodic_bc   = False\n",
    "\n",
    "kernel = 2 # Stride along spatial dimensions\n",
    "output_dim = 1 # output dimension\n",
    " \n",
    "feature_dim = 2\n",
    "\n",
    "logFile = time.strftime(\"%Y%m%d_%H_%M\")+'.txt'\n",
    "makeLogFile(logFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normTensor = 0.5*torch.ones(args.nChannel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.nChannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data processing and loading....\n",
    "trans_valid = transforms.Compose([transforms.Normalize(mean=normTensor,std=normTensor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.aug:\n",
    "\ttrans_train = transforms.Compose([transforms.ToPILImage(),\n",
    "\t\t\t\t\t\t  transforms.RandomHorizontalFlip(),\n",
    "\t\t\t\t\t\t  transforms.RandomVerticalFlip(),\n",
    "\t\t\t\t\t\t  transforms.RandomRotation(20),\n",
    "\t\t\t\t\t\t  transforms.ToTensor(),\n",
    "\t\t\t\t\t\t  transforms.Normalize(mean=normTensor,std=normTensor)])\n",
    "\tprint(\"Using Augmentation....\")\n",
    "else:\n",
    "\ttrans_train = trans_valid\n",
    "\tprint(\"No augmentation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed LIDC data \n",
    "dataset_train = LIDC(split='Train', data_dir=args.data_path, \n",
    "\t\t\t\t\ttransform=trans_train,rater=4)\n",
    "dataset_valid = LIDC(split='Valid', data_dir=args.data_path, \n",
    "\t\t\t\t\ttransform=trans_valid,rater=4)\n",
    "dataset_test = LIDC(split='Test', data_dir=args.data_path, \n",
    "\t\t\t\t\ttransform=trans_valid,rater=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(dataset_train)\n",
    "num_valid = len(dataset_valid)\n",
    "num_test = len(dataset_test)\n",
    "print(\"Num. train = %d, Num. val = %d\"%(num_train,num_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(dataset = dataset_train, drop_last=True, \n",
    "\t\t\t\t\t\t  batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset = dataset_valid, drop_last=True,\n",
    "\t\t\t\t\t\t  batch_size=batch_size, shuffle=False)\n",
    "loader_test = DataLoader(dataset = dataset_test, drop_last=True,\n",
    "\t\t\t\t\t\t batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliaze input dimensions\n",
    "dim = torch.ShortTensor(list(dataset_train[0][0].shape[1:]))\n",
    "nCh = int(dataset_train[0][0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = DenseNet(depth=40, growthRate=12, reduction=0.5,bottleneck=True,nClasses=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "if not args.dense_net:\n",
    "\tprint(\"Using LoTeNet\")\n",
    "\tmodel = loTeNet(input_dim=dim, output_dim=output_dim, \n",
    "\t\t\t\t  nCh=nCh, kernel=kernel,\n",
    "\t\t\t\t  bond_dim=args.bond_dim, feature_dim=feature_dim,\n",
    "\t\t\t\t  adaptive_mode=adaptive_mode, periodic_bc=periodic_bc, virtual_dim=1)\n",
    "else:\n",
    "\tprint(\"Densenet Baseline!\")\n",
    "\tmodel = DenseNet(depth=40, growthRate=12, \n",
    "\t\t\t\t\treduction=0.5,bottleneck=True,nClasses=output_dim)\n",
    "model = loTeNet(input_dim=dim, output_dim=output_dim, \n",
    "\t\t\t\t  nCh=nCh, kernel=kernel,\n",
    "\t\t\t\t  bond_dim=args.bond_dim, feature_dim=feature_dim,\n",
    "\t\t\t\t  adaptive_mode=adaptive_mode, periodic_bc=periodic_bc, virtual_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose loss function and optimizer\n",
    "loss_fun = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, \n",
    "\t\t\t\t\t\t\t weight_decay=args.l2)\n",
    "\n",
    "nParam = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters:%d\"%(nParam))\n",
    "print(f\"Maximum MPS bond dimension = {args.bond_dim}\")\n",
    "with open(logFile,\"a\") as f:\n",
    "\tprint(\"Bond dim: %d\"%(args.bond_dim),file=f)\n",
    "\tprint(\"Number of parameters:%d\"%(nParam),file=f)\n",
    "\n",
    "print(f\"Using Adam w/ learning rate = {args.lr:.1e}\")\n",
    "print(\"Feature_dim: %d, nCh: %d, B:%d\"%(feature_dim,nCh,batch_size))\n",
    "\n",
    "model = model.to(device)\n",
    "nValid = len(loader_valid)\n",
    "nTrain = len(loader_train)\n",
    "nTest = len(loader_test)\n",
    "\n",
    "maxAuc = 0\n",
    "minLoss = 1e3\n",
    "convCheck = 5\n",
    "convIter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start training!\n",
    "for epoch in range(args.num_epochs):\n",
    "\trunning_loss = 0.\n",
    "\trunning_acc = 0.\n",
    "\tt = time.time()\n",
    "\tmodel.train()\n",
    "\tpredsNp = np.zeros(1)\n",
    "\tlabelsNp = np.zeros(1)\n",
    "\n",
    "\tfor i, (inputs, labels) in enumerate(loader_train):\n",
    "\n",
    "\t\tinputs = inputs.to(device)\n",
    "\t\tlabels = labels.to(device)\n",
    "\t\tlabelsNp = np.concatenate((labelsNp, labels.cpu().numpy()))\n",
    "\n",
    "\t\tscores = torch.sigmoid(model(inputs))\n",
    "\n",
    "\t\tpreds = scores\n",
    "\t\tloss = loss_fun(scores, labels)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpredsNp = np.concatenate((predsNp, preds.detach().cpu().numpy()))\n",
    "\t\t\trunning_loss += loss\n",
    "\n",
    "\t\t# Backpropagate and update parameters\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif (i+1) % 5 == 0:\n",
    "\t\t\tprint ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "\t\t\t\t   .format(epoch+1, args.num_epochs, i+1, nTrain, loss.item()))\n",
    "\n",
    "\taccuracy = computeAuc(labelsNp,predsNp)\n",
    "\n",
    "\t# Evaluate on Validation set \n",
    "\twith torch.no_grad():\n",
    "\n",
    "\t\tvl_acc, vl_loss = evaluate(loader_valid)\n",
    "\t\tif vl_acc > maxAuc or vl_loss < minLoss:\n",
    "\t\t\tif vl_loss < minLoss:\n",
    "\t\t\t\tminLoss = vl_loss\n",
    "\t\t\tif vl_acc > maxAuc:\n",
    "\t\t\t\t### Predict on test set\n",
    "\t\t\t\tts_acc, ts_loss = evaluate(loader_test)\n",
    "\t\t\t\tmaxAuc = vl_acc\n",
    "\t\t\t\tprint('New Max: %.4f'%maxAuc)\n",
    "\t\t\t\tprint('Test Set Loss:%.4f\tAuc:%.4f'%(ts_loss, ts_acc))\n",
    "\t\t\t\twith open(logFile,\"a\") as f:\n",
    "\t\t\t\t\tprint('Test Set Loss:%.4f\tAuc:%.4f'%(ts_loss, ts_acc),file=f)\n",
    "\t\t\tconvEpoch = epoch\n",
    "\t\t\tconvIter = 0\n",
    "\t\telse:\n",
    "\t\t\tconvIter += 1\n",
    "\t\tif convIter == convCheck:\n",
    "\t\t\tif not args.dense_net:\n",
    "\t\t\t\tprint(\"MPS\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"DenseNet\")\n",
    "\t\t\tprint(\"Converged at epoch:%d with AUC:%.4f\"%(convEpoch+1,maxAuc))\n",
    "\n",
    "\t\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
